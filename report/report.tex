% !TeX spellcheck = en_US
% arara: pdflatex
% arara: bibtex
% arara: pdflatex
% arara: pdflatex
\documentclass[journal,a4paper,10pt,twoside]{IEEEtran} % with epstopdf, add draft=false
\usepackage[utf8]{inputenc}
\usepackage{times,textcomp,amssymb}
\usepackage[cmex10]{amsmath}
\usepackage[T1]{fontenc}
\usepackage[english]{babel}

\usepackage{breqn,cite}
%\usepackage{epstopdf}
\usepackage[dvipsnames]{xcolor}
\usepackage[pdftex]{graphicx}
\usepackage{subfig}
\usepackage[section]{placeins} % floats never go into next section
%\let\labelindent\relax % Compact lists
\usepackage{array,booktabs,enumitem,balance} % nice rules in tables

% amsmath sets \interdisplaylinepenalty = 10000
% preventing page breaks from occurring within multiline equations
\interdisplaylinepenalty=2500

%tikz figures
\usepackage{tikz}
\usetikzlibrary{automata,positioning,chains,shapes,arrows}
\usepackage{pgfplots}
\usetikzlibrary{plotmarks}
\newlength\fheight
\newlength\fwidth
\pgfplotsset{compat=newest}
\pgfplotsset{plot coordinates/math parser=false}

\newcommand{\EB}[1]{\textit{\color{blue}EB says: #1}}
\newcommand{\FR}[1]{\textit{\color{ForestGreen}FR says: #1}}
\newcommand{\LA}[1]{\textit{\color{orange}LA says: #1}}
\newcommand{\FS}[1]{\textit{\color{red}FS says: #1}}

\usepackage{hyperref}
\definecolor{dkpowder}{rgb}{0,0.2,0.7}
\hypersetup{%
    pdfpagemode  = {UseOutlines},
    bookmarksopen,
    pdfstartview = {FitH},
    colorlinks,
    linkcolor = {dkpowder},
    citecolor = {dkpowder},
    urlcolor  = {dkpowder},
}
\addto\extrasenglish{%
  \renewcommand{\sectionautorefname}{Section}%
}

\pdfminorversion=7 % fixes warnings of eps to pdf included images

%%%%%%%%%%%%%%%%
\begin{document}
\title{A study on the Iterated Prisoner's Dilemma}

\author{%
    \IEEEauthorblockN{Elia Bonetto, Filippo Rigotto, Luca Attanasio and Francesco Savio}

    \IEEEauthorblockA{Department of Information Engineering, University of Padova -- Via Gradenigo, 6/b, 35131 Padova, Italy}
    % \\Email: {\tt\{bonettoe,rigottof,attanasiol,\}@dei.unipd.it}}
}

\maketitle
%%%%%%%%%%

\begin{abstract}
% matching = accoppiamento
In this work the popular Iterated Prisoner's Dilemma game is analyzed in different matching scenarios: (i) the classical version between two players, (ii) a generalization of the classical version between multiple players, (iii) an extension of (ii) allowing the population of players to evolve or (iv) allowing players' strategies to randomly change between rounds, according to a gene representing the grade of cooperation. % (a Nature choice, in Game Theory terms).
Rounds' statistics are collected to have an insight on which is the best strategy, if there is an absolute winner, which is the evolution and which is the players' score of each scenario.
\end{abstract}

\section{Introduction} \label{s:intro}
The Prisoner's Dilemma (PD) is a classical game analyzed in Game Theory which attempts to model social and economical interactions. It is a \textit{dilemma} because, if exploited to explain the emergence of altruism in human society or in general in animal society, it fails badly at a first glance. The game is based on a player that has to make a decision on whether to cooperate or not with its opponent. As we will see shortly, even if the intuition tells us that the best choice is to \textit{cooperate}, the only win-ever strategy in a one-shot game is to \textit{not} cooperate (\textit{defect}).

The rest of the paper is organized as follows.
\autoref{s:game} gives a theoretical and mathematical introduction on the Prisoner's Dilemma problem and on its iterated version, then a base structure that will be used in the remaining of the study is defined.
In \autoref{s:str} we illustrate the possible strategies that have been implemented, while in Sections [\ref{s:IPD2P}, \ref{s:IPDMP}, \ref{s:rIPDMP}, \ref{s:crIPDMP}] we explore the results of the simulations for each study-case.
In \autoref{s:ml} we briefly introduce and review related works that approached the problem using machine learning and artificial intelligence procedures like reinforcement learning and evolutionary algorithms.
At the end, in \autoref{s:conc}, some final considerations summarizing the analysis are presented.
Finally in the Appendix \ref{appendix} all the example tables and images can be found.

All the code, developed in \textit{Python 3}, is available on \href{https://github.com/eliabntt/LaboratoryOfComputationalPhysics/tree/Group9}{GitHub}.

\section{The dilemma explained} \label{s:game}
The classical formulation of the PD is that given two prisoners, in a scenario where their conviction depends on their mutual cooperation, they can either stay silent or fink, respectively cooperate or defect.
Another possible formulation is by the means of a trade-off game called \textit{closed bag exchange}:

\begin{quote}
\textit{Two people meet and exchange closed bags, with the understanding that one of them contains money, and the other contains a purchase. Either player can choose to honor the deal by putting into his or her bag what he or she agreed, or he or she can defect by handing over an empty bag.}
\end{quote}

Mathematically the PD can be expressed with linear algebra. The key component is the \textit{Payoff matrix} $M$, which quantifies the reward of each player depending on whether he/she cooperated or defected:

$$
M = 
\begin{pmatrix} 
R & S \\
T & P 
\end{pmatrix}
$$

where $T$ (Temptation), $R$ (Reward), $S$ (``Sucker's''), $P$ (Punishment) are integer numbers that satisfy the following conditions, as proven by Rapoport~\cite{rapoport}:

$$
T>R>P>S; \quad 2R > T+S 
$$
For example, $T=3$, $R=2$, $P=1$ and $S=0$, or  $T=5$, $R=3$, $P=1$, $S=0$, the default for all our experiments.

$R$ is returned for both players if they both cooperate, $P$ if they both defect; if the two players' actions differ, $S$ is for the player who cooperated and $T$ is for who defected.

Equivalently, each player's choice (move or action) for a single round can be represented by one of the two axis in $\mathbb{R}^2$, i.e. $u_C=\begin{pmatrix} 1 \\ 0 \end{pmatrix}$ or $u_D=\begin{pmatrix} 0 \\ 1 \end{pmatrix}$, where the first axis stands for \textit{Cooperate} and the second for \textit{Defect}. Being $u_1$ and $u_2$ the moves of the first and second player respectively, their rewards $r_1$ and $r_2$ can then be computed as:

$$
r_1 = u_1^T M u_2
\quad
\quad
r_2 = u_2^T M u_1
$$

For a single-shot game, that is, a game which is played only once, the best strategy (choice of action) may seem for both players to cooperate. If both players cooperate, this leads to a good payoff which maximizes the global outcome, evaluated as the sum of the payoffs for each of them. This is indeed the \textit{Pareto dominating strategy}.

However if a player cooperates, he has an incentive to deviate from his choice and so to betray the other player and defect as this leads to a better payoff for himself, and this statement is true for both players. 
Given the fact that both players are rational and fully aware of the rules of the game (they have \textit{common knowledge}, using Game Theory terms) and that they move simultaneously, both of them will easily conclude that the best way of acting is to defect as this would lead to a slightly lower payoff if the opponent defects (minor punishment) but a higher one if the other player chooses to cooperate.
Following this, the only possible reasonable conclusion is that the only \textit{Nash Equilibrium}, or the only way to win this game in a single-shot scenario, is to always defect.
This is not \textit{Pareto optimal} but playing cooperate, as it has just been shown, is not feasible: the only strategy in which nobody wants to deviate is to defect, as also noted by Fogel~\cite{fogelEvolvingBehaviors}.

This reasoning is no longer true if repeated games are considered. In particular this is the case of the multiplayer Iterated Prisoner Dilemma (IPD), since time and memory (history) must be considered.
Colman supported this indicating that the 2-player IPD is different from the generalized N-player version, and that strategies that work well in the first scenario may fail in large groups.~\cite{yao1994experimental}
In addition, strategies like random, grim triggers or Tit For (Two) Tat can be introduced.
Moreover, in this game, or more generally in iterated games, the notions of Nash Equilibrium, Pareto optimal or evolutionary stable strategies\footnote{A strategy is said to be \textit{evolutionary stable} if it cannot be overwhelmed by the joint effect of two or more competing strategies. In fact, Lorberbaum, Boyd, Farrell and Ware proved that no pure or mixed strategy is ev. stable in the long term, if future moves are discounted (see~\cite{lorb94}).} do not suggest new, efficient and interesting strategies. \cite{mathieu2017}
Winning a game in this setup simply means achieving a better payoff in the long run.
%In Section~\ref{s:IPD2P} we will see a simple one-vs-one game, iterated through time, while in the subsequent cases population and other dynamics will be involved.

\section{Strategies} \label{s:str}
The strategy is represented as a function which outputs either $u_C$ or $u_D$. Based on the strategy, such function might depend on one or both players' history of moves, or on the number of moves played till that moment and so on.
The strategy is based on a probability density function. In this project both probabilistic and deterministic strategies are used.

The strategies based on probability are:
\begin{description}
    \item[Nice guy] always cooperate (function's output is always $u_C$).
    \item[Bad guy] always defect (function's output is always $u_D$).
    \item[Mainly nice] randomly defect $k\%$ of the times, $k<50$.% and cooperate $100-k\%$, with $k<50$.
    \item[Mainly bad] randomly defect $k\%$ of the times, $k>50$.% and cooperate $100-k\%$, with $k>50$.
    \item[Indifferent] randomly defect half $(k=50\%)$ of the times.
\end{description}

The deterministic strategies are:
\begin{description}
    \item[Tit-for-Tat (TfT)] start by cooperating the first time, then repeat opponent's previous move.
    \item[Tit-for-Two-Tat (Tf2T)] cooperate the first two times, then defect only if the opponent defected last two times.
    \item[Grim-Trigger (GrT)] always cooperate until the opponent's first defect move, then always defect. 
\end{description}

The strategies are considered static in case they apply the same move at each iteration as in \textit{Nice guy}, \textit{Bad guy} and dynamic elsewhere.
These players' strategies are generally fixed in time, i.e. a player cannot change its strategy between rounds, unless specifically requested by the study case rules.% (for an example, see Section~\ref{s:crIPDMP}).
Many more and much complicated strategy can be analyzed and our implementation is open and structured such that adding one would be easy and immediate changing just a couple of code lines.

\section{Two players IPD} \label{s:IPD2P}
In this section the IPD intercourse between two players is evaluated: each player has an assigned strategy, he is unaware of the opponent's strategy and he plays accordingly to his strategy definition without the possibility to change it. Both players know only their respective history of moves. Each game is repeated for a fixed number of rounds, unknown to the two players. The main metric evaluated as output of this game is who wins, or in other words who achieves a higher payoff at the end of the round.

The number of iterations was fixed to \texttt{NUM\_ITER = 50} and can be modified by the means of a simple option when launching the program. This could also be seen as the number of moves during the match, and is a factor which is unknown to the players otherwise a smart player could adopt a supposed optimal strategy: for example against a player adopting the \textit{Nice}(or \textit{TfT}) strategy the best choice of action would be to cooperate in all but the last round, so gaining advantage from knowing the number of runs. 

The results do not depend on this value if using only deterministic strategies, instead it will influence slightly the random ones. Note that in every simulation we fix a \texttt{seed} value to have reproducible results as we are using pseudo-random generators.
\\

All the possible combinations between players assigned to strategies presented in \autoref{s:str} are evaluated, including the case in which the player plays against itself (or, equivalently, against a player with the same strategy).
This is a simple repetition of the single-shot game with the addition of memory and the possibility to add probabilistic and more elaborated strategies, as it has already be seen.

Since population is not a concern in this study-case (which is a single A vs B game), the winning strategy in all cases is to \textit{not} cooperate, or, in other terms, the \textit{Always Bad guy} strategy, as expected.

In fact in all scenarios, \textit{Bad guy} has a higher reward than the opponent as in Figures~[\ref{fig:badvstft},\ref{fig:badvsnice},\ref{fig:badvsmainlybad}]. 
On the other hand, when \textit{Bad guy} plays against \textit{Bad guy} as in \autoref{fig:badvsbad}, or similarly against \textit{Mainly bad} (\autoref{fig:badvsmainlybad}), this leads to the same cumulative reward for both players or almost the same respectively. The obtained reward is not as good as if players were playing against (mainly) nice strategies. This is a first important insight that verifies and points out what has been seen from a theoretical point of view earlier: defect is always a winning strategy, but it may be non-optimal; on the other hand, if both are cooperating, each of them would get an advantage if they defect against the other.

\begin{figure}[!ht]
    \centering
    \includegraphics[width=1\columnwidth]{../img/ipd2p/ipd2p-rewards-Bad-Bad}
    \caption{Bad vs Bad}
    \label{fig:badvsbad}
\end{figure}

\begin{figure}[!ht]
    \centering
    \includegraphics[width=1\columnwidth]{../img/ipd2p/ipd2p-rewards-Bad-TitForTat}
    \caption{Bad vs TfT}
    \label{fig:badvstft}
\end{figure}

\begin{figure}[!ht]
    \centering
    \includegraphics[width=1\columnwidth]{../img/ipd2p/ipd2p-rewards-Bad-Nice}
    \caption{Bad vs Nice}
    \label{fig:badvsnice}
\end{figure}

\begin{figure}[!ht]
    \centering
    \includegraphics[width=1\columnwidth]{../img/ipd2p/ipd2p-rewards-Bad-MainlyBad(k=72)}
    \caption{Bad vs Mainly bad}
    \label{fig:badvsmainlybad}
\end{figure}

Taking a closer look, the combination of \textit{Nice} and one between \textit{TfT} or \textit{Nice} leads to better payoffs at the end of the runs as in Figures~[\ref{fig:tftvsindiff},\ref{fig:nicevsnice},\ref{fig:nicevstft}]. The idea behind this is that both players are getting the highest reward, not just one of them, and these choices are better compared to the \textit{Bad}-\textit{Nice} combination.
These are isolated cases that are being considered just because it is a study case, since the only strategy that wins against all the others and draws against itself is the \textit{Bad} one and so this is the strategy a smart player should choose, reminding both players have \textit{common knowledge}.

\begin{figure}[!ht]
    \centering
    \includegraphics[width=1\columnwidth]{../img/ipd2p/ipd2p-rewards-Indifferent-TitForTat}
    \caption{TfT vs Indifferent}
    \label{fig:tftvsindiff}
\end{figure}

\begin{figure}[!ht]
    \centering
    \includegraphics[width=1\columnwidth]{../img/ipd2p/ipd2p-rewards-Nice-Nice}
    \caption{Nice vs Nice}
    \label{fig:nicevsnice}
\end{figure}

\begin{figure}[!ht]
    \centering
    \includegraphics[width=1\columnwidth]{../img/ipd2p/ipd2p-rewards-TitForTat-Nice}
    \caption{Nice vs TfT}
    \label{fig:nicevstft}
\end{figure}

\begin{figure}[!ht]
    \centering
    \includegraphics[width=1\columnwidth]{../img/ipd2p/ipd2p-rewards-TitForTat-TitForTat}
    \caption{TfT vs TfT}
    \label{fig:tftvstft}
\end{figure}

The \textit{TfT} strategy is interesting, because \textit{TfT} leads to almost the same cumulative reward as the opponent, and it is highly adaptive, even if fast-forgiving if it goes against a mainly bad strategy.
Or, in other words, \textit{TfT} is robust because it never defects first and never takes advantage for more than one iteration at a time.~\cite{fogelEvolvingBehaviors}
\textit{TfT} was also reported to be the winner over all the strategies analyzed by Axelrod in his extensive tests based on tournaments and evolutionary competition on the IPD, thoroughly described in~\cite{axelrod1981evolution,axelrod1984evolution} and taken up in~\cite{mathieu2017}.
The strategy was proposed to him by Anatol Rapoport.
A player might choose this move if he/she wants almost the same reward as the opponent. This strategy is even more interesting in the following sections.

In addition to these considerations simulations were performed multiple times to get insights of the mean and variance of the rewards that rule these games. Obviously the static strategies (as the \textit{Nice-Nice}, \autoref{fig:boxnn}), or the non-triggering ones, or the the ones without variations have a constant mean and $0$ std. On the other hand, it is interesting to notice that random strategies have a non null variance as shown in \textit{Mainly Bad-TitForTat}, \autoref{fig:boxmbvtft}. Anyway this does not imply that \textit{TitForTat} could ever win against such a strategy, it is only pointing out that there is a variation on subsequent runs based on the randomness of at least one of the two players: the \textit{TfT} strategy is a reactive strategy so it will be always ``late'', meaning that a player applying it will have always at most the same points of the opponent at the end of the game.

\begin{figure}[!ht]
    \centering
    \includegraphics[width=1\columnwidth]{../img/ipd2p/ipd2p-boxplot-Nice-Nice}
    \caption{Nice vs Nice}
    \label{fig:boxnn}
\end{figure}

\begin{figure}[!ht]
    \centering
    \includegraphics[width=1\columnwidth]{../img/ipd2p/ipd2p-boxplot-TitForTat-MainlyBad(k=72)}
    \caption{TfT vs Mainly Bad}
    \label{fig:boxmbvtft}
\end{figure}

More insights about this part, including the complete collection of the generated pictures, can be found in the repository and in \autoref{tab:ipd2p}, where collected statistics are presented.
Furthermore, it can be seen that the only strategies that reach $0$ as a final payoff are the \textit{Nice} ones, while the \textit{TfT, Tf2T, GrT, Bad} have a higher minimum value.

It is impossible to make the optimal score against all strategies. The most intuitive reason is a consequence of the first move: to play optimally against a \textit{Bad} guy it is necessary to defect at the first round, to play optimally against \textit{GrT} it is necessary to cooperate until the last round where you should defect.~\cite{mathieu2017}
But the number of rounds is unknown to the players and they should know in advance the type of the opponent: this would allow them to adapt their strategy, but is not allowed by the rules of the game. 
\\
Finally we have introduced other two metrics: \textit{yield} and \textit{achieve}(\autoref{tab:ipd2p}). The first one can be expressed as $yield(p_1) = \frac{points(p_1)}{optimal\ points(p_2)}$, where $points(p_1)$ is the number of points at the end of the round and $optimal\ points(p_2)$ is the maximum that $p_1$ could achieve if it knew $p_2$ moves in advance, while the latter can be expressed as $achieve(p_1) = \frac{points(p_1)}{hoped\ points}$ and represents how far the player is with respect to his best expectation.

The \textit{yield} metric backs up our claim that \textit{Bad} is the only win-always strategy as it's the only one that gives a stunning $100\%$ for all the matches playing a perfect move on every opponent. In other words, a player using it does not need to know in advance which strategy the opponent is adopting. Moreover this metric points out how \textit{TfT, Tf2T, GrT} strategies are more resilient. In particular \textit{GrT} reaches scores over $80\%$ even against the \textit{(Mainly)Bad} ones.

The \textit{achieve} column is a coupled metric, taking into account both player A and B. We notice once again that, ruling out the same-strategy couples, \textit{TfT, Tf2T} and \textit{GrT} strategies achieve results (almost)always comparable with the opponents, meaning they are at least as good as them.

\textbf{TODO THE AVERAGE FOR EACH KIND OF PLAYER FOR EACH METRIC(AKA BAD avg yield avg achieve)}

\section{Multiple players IPD - Round-robin scheme} \label{s:IPDMP}
The IPD with \textit{round-robin} (RR) scheme, used to match-up the opponents, consists in a number of players, with multiple strategies, not necessarily different, with each player playing once against each other for a fixed \texttt{NUM\_ITER} of times (in our simulation this value is set to \texttt{50}).

Each player chooses its fixed strategy at the beginning of the tournament and holds it throughout the course of the whole tournament. Nobody knows the strategies of the other players.

In short, it is a variation of the previous case, in which multiple players, with possibly different strategies, play in a RR way. The variation consists in the fact that a single player will win the tournament if at the end of it, he has the highest cumulative payoff. This means that, since there are $\frac{N[N-1]}{2}$ possible couples of players and $I$ iterations of the game at the end we will have a total of $I\cdot\frac{N[N-1]}{2}$ matches. From the perspective of each single player the total number of games is simply $I\cdot(N-1)$.\\

Tournament statistics, like points and counts of cooperation and defection moves, along with the percentage of cooperation, are shown in \autoref{tab:ipdmp50}.

As a validation proof, our results have been compared to the ones obtained from the Axelrod Tournament Demo software, \cite{demosw} but it is noted that this software does not implement all the strategies considered in this work. For example, \textit{GrT} is \textit{Spiteful}, but the software cannot set \textit{Mainly Bad/Good} strategies with a given probability of cooperating for which a \textit{Random} agent is used as a substitute. Thus, slightly different outcomes were foreseen due to this constraint but, despite of that, the evolution of the tournament is quite similar between the two simulations.
Doing a simulation with only deterministic strategies leads indeed to the same results. \textbf{TODO FORCE such a config and put images in appendix}

\begin{figure}[!ht]
    \centering
	\includegraphics[width=1\columnwidth]{../img/ipdmp/ipdmp-evolution-of-game-8}
	\caption{8 players, evolution of the game}
	\label{fig:boxIPDMPevo8}
\end{figure}

\begin{figure}[!ht]
    \centering
	\includegraphics[width=.9\columnwidth]{../img/ipdmp/ipdmp8-plot-det}
	\caption{8 players, evolution -- software results \cite{demosw}}
	\label{fig:boxIPDMPevo8sw}
\end{figure}

\begin{figure}[!ht]
    \centering
	\includegraphics[width=.9\columnwidth]{../img/ipdmp/ipdmp50-plot-det}
	\caption{50 players, evolution -- software results \cite{demosw}}
	\label{fig:boxIPDMPevo50sw}
\end{figure}

\begin{figure}[!ht]
    \centering
    \includegraphics[width=1\columnwidth]{../img/ipdmp/ipdmp-evolution-of-game-50}
    \caption{50 players, evolution of the game}
    \label{fig:boxIPDMPevo}
\end{figure}

\begin{figure}[!ht]
    \centering
    \includegraphics[width=1\columnwidth]{../img/ipdmp/ipdmp-boxplot-single-match-50}
    \caption{50 players, boxplot of a single match}
    \label{fig:boxIPDMPsingle}
\end{figure}

\begin{figure}[!ht]
    \centering
    \includegraphics[width=1\columnwidth]{../img/ipdmp/ipdmp-boxplot-final-points-50}
    \caption{50 players, boxplot of the final points}
    \label{fig:boxIPDMPfinal}
\end{figure}

\textbf{TODO REWRITE}
The results of the tournament (and of the following study-cases) depend on the initial population and the balance between the amount of \textit{Bad} and \textit{TfT} players. The best strategy overall seems to be the \textit{TfT} one.
These outcomes may also be found in~\cite{mathieu2017}.

It is noticed that in an 8 players game (Figure \ref{fig:boxIPDMPevo8}) the best overall strategy is \textit{Bad guy}. This behaviour is linked to a subtle balance between ``good'' and ``bad'' strategies. As pointed out previously, \textit{TfT} is a reactive strategy to win a tournament and it should be coupled with enough (mainly) good players, otherwise the bad players will easily overcome and win the tournament. In the end, after various tries, it is found that a good setup includes more ``good'' strategies with respect to the ``bad'' strategies in order to have a \textit{TfT} winner. This consideration is not available in literature but it is important and worth noticing it, although it can be explained by the game's intrinsic. This statement helps to interpret results and assign them the right meaning. \textit{TfT} is considered one of the best strategies to win the tournament since it surely is one of the best strategies to maximize the score. However, \textit{TfT} wins only in specific tournaments cases depending on the initial population.

In each tournament, variations of the results can be obtained by running the simulations multiple times and generating boxplots.
Since random strategies have been introduced, the result of a one-shot complete game may differ with respect to the average results; a simple example of this is the 8 players game where on average the \textit{Bad} guy wins but there may be runs where the \textit{GrT} strategy wins (as evidenced in \autoref{fig:boxIPDMPevo8}).

Another example is the 50 players game, in which players are assigned to a random strategy \autoref{fig:boxIPDMPfinal}. In this repeated tournament, we notice that the winning strategy is always \textit{TfT}. Multiple \textit{(mainly) bad} strategies achieve a high score similarly to other \textit{TfT} strategies. Just below them, in this simulation the results of \textit{GrT} can be compared to an \textit{indifferent}. At last, \textit{(mainly) nice} strategies achieve the lowest scores.

Running simulations with different strategies and a different initial population (i.e. 20 or 30 players), we notice the results are different especially since the balance between the number of \textit{(mainly) bad} and \textit{(mainly) nice} guys differs from the previously analyzed scenarios. In these cases \textit{TfT} doesn't win, but performs almost as good as \textit{(mainly) bad} guys.

\textbf{TODO ADD YIELD ACHIEVE WHEN GREEN LIGHT}

\section{Repeated multiple players IPD} \label{s:rIPDMP}
The previously defined MPIPD tournament is then iterated many times.
Two separated scenarios have been developed to study the behaviour and the evolution of the populations and the convergence speed by simulations. A population is said to be converged if more than $3/4$ of it has the same strategy type at the end of a complete round. The base rules are the same as pointed out in the previous sections (\textit{common knowledge},  etc.).
This schema is identified as a \textit{Repeated multiple players IPD} (rMPIPD).

\subsection{Static Population}
In this case the number of players is fixed. Each player implements a strategy choosing it with equal probability from the strategies set. At the end of each round the population is sorted with respect to the cumulative payoff and a fixed percentage (in our simulation $30\%$) is ``doubled'', so for each player in it another player with the same strategy is added to the population. Finally the players in the last $x\%$ of the chart are then removed from the game regardless of their strategies. In this way the total number of players is ensured to be static and then the convergence of the population through consecutive rounds can be studied. After this the scores are resetted and the tournament can restart.

If convergence is not reached after a maximum number of repetitions, execution of the program is stopped.
This method also resembles how Axelrod made his tests~\cite[\S 2.6]{mathieu2017}~\cite{axelrod1984evolution}.

Figures~[\ref{fig:constR},\ref{fig:constFI},\ref{fig:constLI}] show the evolution of a population of 50 players over some iterations.
Details on the evolution of the population, grouped by strategy type, are presented in \autoref{tab:ripdmp-const}.
It can be easily seen how the \textit{TfT} strategy very quickly outpaces the others: it requires only $4$ iterations for \textit{TfT} to overcome the other strategies.

\begin{figure}[!ht]
    \centering
    \includegraphics[width=1\columnwidth]{../img/ripdmp-const/ripdmp-evolution-const-pop-50}
    \caption{Evolution of rIPDMP, constant population of 50}
    \label{fig:constR}
\end{figure}

\begin{figure}[!ht]
    \centering
    \includegraphics[width=1\columnwidth]{../img/ripdmp-const/ripdmp-scores-const-pop-50-r0}
    \caption{First iteration scores ($it=0$)}
    \label{fig:constFI}
\end{figure}

\begin{figure}[!ht]
    \centering
    \includegraphics[width=1\columnwidth]{../img/ripdmp-const/ripdmp-scores-const-pop-50-r3}
    \caption{Last iteration scores ($it=3$)}
    \label{fig:constLI}
\end{figure}

\subsection{Increasing Population}
In this case the number of players (population) is increased at each iteration. Three different ways of adding population between rounds have been implemented; after each round a player has a certain probability based on his ranking to have a child of the same type:
\begin{enumerate}
    \item The probability is $p(i)=1- i\ /\ num\_players$ where $i$ is the position reached by the player. The winner of the round is indeed doubled, because $p(0)=1$, while the looser is not, as $p(last)=0$.
    For each player a random number $d$ is drawn, according to a uniform probability distribution, and compared with $p(i)$. If $p(i)$ is greater than $d$ the player is effectively doubled, otherwise not.
    \item The ordered population is splitted into three sets of equal size $A,B,C$. For each player in the population, a random number $d$ is drawn and its strategy is doubled if:
    \begin{itemize}
        \item $d>0.2$ if the player belongs to $A$
        \item $d>0.5$ if the player belongs to $B$
        \item $d>0.8$ if the player belongs to $C$
    \end{itemize}
    This is an alternative way to promote best strategies, due to the higher probability of being doubled, and obstruct less performant players, whose total number does not increase significantly.
    \item A player's score is defined as its obtained points divided by the maximum obtained score in the whole population. The player's strategy is doubled if a drawn random number is greater than its score.
\end{enumerate}

In our software the first of the three proposed methods is used by default. Other methods can be set using the \texttt{ALTERNATIVE} variable.

Figures~[\ref{fig:incrR},\ref{fig:incrFI},\ref{fig:incrMI},\ref{fig:incrLI}] show the evolution of a population of 50 players over four iterations. In this case convergence is not reached at the fifth iteration, since the population is increasing, but the simulation still exhibits the same behaviour. The \textit{TfT} strategy is getting stronger and stronger. It is concluded that, in the future, i.e. evaluating the problem with more iterations, the population will increase with similar behaviour and converge to the \textit{TfT} strategy.

\begin{figure}[!ht]
    \centering
    \includegraphics[width=1\columnwidth]{../img/ripdmp-incr/ripdmp-evolution-increasing-pop-50}
    \caption{Evolution of rIPDMP, increasing population of 50}
    \label{fig:incrR}
\end{figure}

\begin{figure}[!ht]
    \centering
    \includegraphics[width=1\columnwidth]{../img/ripdmp-incr/ripdmp-scores-increasing-pop-50-r0}
    \caption{First iteration scores ($it=0$)}
    \label{fig:incrFI}
\end{figure}

\begin{figure}[!ht]
    \centering
    \includegraphics[width=1\columnwidth]{../img/ripdmp-incr/ripdmp-scores-increasing-pop-50-r2}
    \caption{Middle iteration scores ($it=2$)}
    \label{fig:incrMI}
\end{figure}

\begin{figure}[!ht]
    \centering
    \includegraphics[width=1\columnwidth]{../img/ripdmp-incr/ripdmp-scores-increasing-pop-50-r4}
    \caption{Last iteration scores ($it=4$)}
    \label{fig:incrLI}
\end{figure}

It is interesting to note how Wu and Axelrod~\cite{IPDnoise} exploit this behaviour to react to noise in the game: just by slightly altering the \textit{TfT} strategy in both directions, adding generosity (some percentage of opponent's defections go unpunished) or contrition (avoid responding to a defect move when a player's previous defection was unintended) the ``error'' can be quickly recovered and cooperation can be successfully restored. Note, however, that in this work these two variations of the \textit{TfT} strategy are not implemented.

With increasing population note also that the simulation times grows exponentially since for each player we add, as we have seen in \autoref{s:IPDMP}, we are adding $I\cdot(N'-1)$ rounds and moreover $I\cdot(N'-1)\cdot{NUM\_ITER}$ iterations(or rounds played): this easily explode. Despite that, and the dependence with respect to the initial population, also pointed out in \autoref{s:IPDMP} and \autoref{s:rIPDMP}, we can see that what is suggested from the constant population is preserved also in this case.

\section{rMPIPD with changing strategies} \label{s:crIPDMP}
A step further is made by allowing players to change their strategies in the rIPDMP setup, from which the main structure is unaltered.
Each player has a gene $c$, representing his attitude to cooperate. This value mutates randomly with a uniform distribution between $0$ and $1$.
We observe that in this section \textit{GrT}, \textit{TfT} and \textit{Tf2T} are \textit{jolly strategies}. \textit{GrT} is triggered randomly only if a strategy is between \textit{Indifferent} and a \textit{Mainly bad (k = 60)}, while the remaining two are triggered randomly. It is known that probability strategies from the most cooperative to the least coperative are in this order: \textit{Nice}, \textit{Maninly nice}, \textit{Mainly bad}, \textit{Bad}.
Two alternatives are proposed to change the strategy after each round.
\begin{enumerate}
    \item If the absolute value of the difference between the old $c$ and the new $c$ is greater than a threshold (set to $0.1$) then I proceed. If the old $c$ is greater than the new $c$ I am going towards a less cooperative behaviour. Otherwise, I am going to a more cooperative behaviour. 
    In case I am going towards a less cooperative behaviour I choose the closest less cooperative strategy next to the actual strategy drawn from a random set of strategies. Otherwise, I choose the closests more cooperative strategy next to the actual strategy.

    \item The change of strategy is again based on players' ranking and probability. First of all the value of $c$ is updated: a strategy that gives good results is enhanced (sets a higher $c$) and a bad strategy leads to a change towards the ``opposite'' direction (sets a lower $c$). If I am more probably going towards a less cooperative behaviour then a random strategy, that satisfies the less cooperative behaviour, is picked up from a random list. Otherwise, a more cooperative strategy is picked.
\end{enumerate}
\textbf{Left loose on purpose because of the todo in change_strategy calling for review. Will be more detailed}

\section{Machine Learning approaches} \label{s:ml}
Since the birth of this subject studies have been developed to find some pattern that can be exploited by learning algorithms.
By the formulation of the IPD game, if such a pattern exists it has to be learned using unsupervised techniques, since the final outcome of the players depends on their actions.

It is desired for players to learn to cooperate --- equivalently, to enhance their altruism --- and here the aim is to do so by adaptively \textit{learning} a strategy. An obstacle to this is that in the PD the Nash Equilibrium solution (as stated before, to defect) is not a desirable learning target. \cite{coopSeqRL}

Reinforcement learning (RL) is an unsupervised learning technique where the system must select an output (in the case of the PD, an action: cooperate or defect) for which it receives a scalar evaluation. RL requires to find the best output for any given input, and is based on the idea that the tendency to produce an action should be strengthened if the action led to favorable results, and weakened otherwise. \cite{sandholmRL}

Sandholm and Crites \cite{sandholmRL} employed recurrent neural networks (RNN) and the Q-learning algorithm, a particular RL procedure that works by estimating the value of state-action pairs, to train agents to play against the \textit{TfT} strategy and against and unknown opponent. While the first task was easily learned, the second one proved to be more difficult due to non-stationary behaviour and lack of \textit{a priori} knowledge of a policy to encourage cooperation.
More recently, Wang \cite{kedaoRL} extended this study with newly developed structures for the RNN part, to test both finite and infinite iterations setups. However, his tests led to pretty much the same results Sandholm previously obtained.
Finally, evolutionary and particle swarm algorithms are used by Harper \textit{et al.} in a very extensive study \cite{plosRLdominant} to train strategies to perform well against over 170 distinct opponents even in noisy tournaments.

What is noted from the literature is that it is an easy task for a player to learn to compete against a deterministic player, while it is indeed difficult to generalize to an opponent with unspecified behaviour.

We believe that this is a very particular problem where given the unknown and the restrictions of the game cannot be fully solved by machine-learning mean since the rule of the game restrict it in a very simple but very powerful area where this methods gain their strengths: information. We can't allow the algorithm to know the opponents moves in advance, or their type, or how much the game will going on or any other information that would be viable and helpful to solve the game. Adding to this the intrinsic "incoherence" between rational behaviour and optimal outcome(Nash Equilibrium vs Pareto efficiency), make the task extremely difficult if not even impossible.
\textbf{todo review and maybe add/rephrase non-cited content}

\section{Conclusions and future work} \label{s:conc}
\textbf{TODO some conclusion to blend with introduction}

This work can easily be extended in many ways in a future development: for example, a lot of new and more complex strategies were created since the original tournaments made by Axelrod and these may be incorporated in the analysis aside of the available ones.
The Axelrod library, \cite{Knight2016Axel,axel-lib} written in Python and also used by \cite{plosRLdominant}, contains more than 270 standard, deterministic and learning-based strategies, and is now the reference framework to study the prisoner's dilemma.
Moreover, research may be directed to find and implement different features or network structures that achieve better results with reinforcement learning.

\textbf{TODO: PLACE IN THE CORRECT POSITION}
When paired with a mindless strategy like probability strategies, \textit{TfT} sinks to its opponent's level. For this reason, \textit{TfT} isn't the ``best'' strategy. In addition \textit{TfT} aims at winning the whole tournament and not single rounds.

Finally, it must again be pointed out that there is no ``best'' strategy for prisoner's dilemma. Each individual strategy will work better when matched against a ``worse'' strategy. In order to win, a player should figure out his opponent's strategy and then pick a strategy that is best suited for the situation.

\balance
\bibliographystyle{IEEEtran}
\bibliography{report}

\onecolumn
\appendix[Additional figures and tables of tournament results]\label{appendix}

%\appendices
%\section{Additional figures} \label{a:fig}
%\section{Tables of tournament results} \label{a:tab} 

\begin{table}[ht]
	\caption{2-players IPD, statistics}
	\label{tab:ipd2p}
	\centering
	\begin{tabular}{ll|rrrr|rrrr}	\toprule
		\multicolumn{2}{c}{Strategies}        & \multicolumn{4}{c}{Scores Player 1} & \multicolumn{4}{c}{Scores Player 2} \\
		Player 1          & Player 2          &   avg &   std &  yield &    achieve &   avg &   std &  yield &    achieve \\ \midrule
		Bad               & Bad               &  50.0 &  0.00 & 100.00 &      20.00 &  50.0 &  0.00 & 100.00 &      20.00 \\
		Bad               & TitFor2Tat        &  58.0 &  0.00 & 100.00 &      23.20 &  48.0 &  0.00 &  96.00 &      19.51 \\
		Bad               & GrimTrigger       &  54.0 &  0.00 & 100.00 &      21.60 &  49.0 &  0.00 &  98.00 &      19.76 \\
		Bad               & Indifferent       & 146.0 & 11.03 & 100.00 &      58.40 &  26.0 &  2.76 &  52.00 &      12.84 \\
		Bad               & MainlyNice (k=27) & 205.6 & 11.66 & 100.00 &      82.24 &  11.1 &  2.91 &  22.20 &       6.39 \\
		Bad               & TitForTat         &  54.0 &  0.00 & 100.00 &      21.60 &  49.0 &  0.00 &  98.00 &      19.76 \\
		Bad               & MainlyBad (k=72)  & 102.0 & 15.39 & 100.00 &      40.80 &  37.0 &  3.85 &  74.00 &      16.48 \\
		Bad               & Nice              & 250.0 &  0.00 & 100.00 &     100.00 &   0.0 &  0.00 &   0.00 &       0.00 \\
		TitFor2Tat        & TitFor2Tat        & 150.0 &  0.00 &  60.00 &     100.00 & 150.0 &  0.00 &  60.00 &     100.00 \\
		TitFor2Tat        & GrimTrigger       & 150.0 &  0.00 &  60.00 &     100.00 & 150.0 &  0.00 &  60.00 &     100.00 \\
		TitFor2Tat        & Indifferent       &  91.4 &  4.52 &  62.50 &      52.76 & 160.4 & 11.19 &  79.57 &      79.78 \\
		TitFor2Tat        & MainlyNice (k=27) & 110.3 &  8.54 &  59.34 &      68.97 & 164.3 &  5.80 &  71.78 &      90.45 \\
		TitFor2Tat        & TitForTat         & 150.0 &  0.00 &  60.00 &     100.00 & 150.0 &  0.00 &  60.00 &     100.00 \\
		TitFor2Tat        & MainlyBad (k=72)  &  73.1 &  4.18 &  71.23 &      36.40 & 127.1 & 14.69 &  87.02 &      57.08 \\
		TitFor2Tat        & Nice              & 150.0 &  0.00 &  60.00 &     100.00 & 150.0 &  0.00 &  60.00 &     100.00 \\
		GrimTrigger       & GrimTrigger       & 150.0 &  0.00 &  60.00 &     100.00 & 150.0 &  0.00 &  60.00 &     100.00 \\
		GrimTrigger       & Indifferent       & 150.2 & 11.79 &  99.07 &      60.56 &  28.2 &  2.96 &  52.22 &      14.13 \\
		GrimTrigger       & MainlyNice (k=27) & 198.0 &  9.15 &  99.20 &      79.90 &  16.5 &  2.46 &  30.31 &       9.39 \\
		GrimTrigger       & TitForTat         & 150.0 &  0.00 &  60.00 &     100.00 & 150.0 &  0.00 &  60.00 &     100.00 \\
		GrimTrigger       & MainlyBad (k=72)  & 101.7 & 12.24 &  98.52 &      41.04 &  40.7 &  3.49 &  74.78 &      18.19 \\
		GrimTrigger       & Nice              & 150.0 &  0.00 &  60.00 &     100.00 & 150.0 &  0.00 &  60.00 &     100.00 \\
		Indifferent       & Indifferent       & 112.3 & 10.21 &  74.56 &      56.17 & 112.3 & 10.21 &  74.56 &      56.17 \\
		Indifferent       & MainlyNice (k=27) & 150.3 & 16.27 &  77.36 &      75.42 &  97.3 & 12.86 &  64.02 &      54.59 \\
		Indifferent       & TitForTat         & 117.5 &  7.27 &  73.92 &      60.08 & 115.5 &  7.99 &  73.39 &      59.30 \\
		Indifferent       & MainlyBad (k=72)  &  77.5 & 14.15 &  70.26 &      38.68 & 126.5 & 11.59 &  84.81 &      57.50 \\
		Indifferent       & Nice              & 200.8 &  5.00 &  80.32 &     100.00 &  73.8 &  7.49 &  49.62 &      49.20 \\
		MainlyNice (k=27) & MainlyNice (k=27) & 132.7 & 12.88 &  67.31 &      75.06 & 132.7 & 12.88 &  67.31 &      75.06 \\
		MainlyNice (k=27) & TitForTat         & 135.6 &  3.83 &  67.33 &      77.61 & 133.6 &  4.80 &  66.86 &      76.82 \\
		MainlyNice (k=27) & MainlyBad (k=72)  &  61.9 &  8.84 &  56.42 &      34.92 & 170.4 & 10.44 &  86.87 &      77.27 \\
		MainlyNice (k=27) & Nice              & 179.8 &  4.33 &  71.92 &     100.00 & 105.3 &  6.50 &  55.26 &      70.20 \\
		TitForTat         & TitForTat         & 150.0 &  0.00 &  60.00 &     100.00 & 150.0 &  0.00 &  60.00 &     100.00 \\
		TitForTat         & MainlyBad (k=72)  &  87.6 &  7.05 &  81.94 &      40.00 &  92.1 &  7.03 &  83.34 &      41.70 \\
		TitForTat         & Nice              & 150.0 &  0.00 &  60.00 &     100.00 & 150.0 &  0.00 &  60.00 &     100.00 \\
		MainlyBad (k=72)  & MainlyBad (k=72)  &  88.5 & 12.09 &  81.42 &      40.03 &  88.5 & 12.09 &  81.42 &      40.03 \\
		MainlyBad (k=72)  & Nice              & 222.0 &  8.20 &  88.80 &     100.00 &  42.0 & 12.30 &  38.68 &      28.00 \\
		Nice              & Nice              & 150.0 &  0.00 &  60.00 &     100.00 & 150.0 &  0.00 &  60.00 &     100.00 \\ \bottomrule
	\end{tabular}
\end{table}

\begin{table}[ht]
	\caption{50-players IPD, sorted by points, statistics}
	\label{tab:ipdmp50}
	\centering
	\begin{tabular}{l|rrr|rrrrr} \toprule
		                  & \multicolumn{3}{c}{Points} & \multicolumn{2}{c}{Coop. count} & \multicolumn{2}{c}{Defect count} &        \\
		Strategy          &    avg &   std &     yield &    avg &                    std &    avg &                     std &   C \% \\ \midrule
		TitForTat         & 5884.6 & 26.69 &     71.37 & 1664.8 &                  10.76 &  785.2 &                   10.76 &  67.95 \\
		MainlyBad (k=78)  & 5449.6 & 39.35 &     85.00 &  545.4 &                  22.11 & 1904.6 &                   22.11 &  22.26 \\
		MainlyBad (k=85)  & 5422.8 & 58.42 &     89.72 &  373.9 &                  17.92 & 2076.1 &                   17.92 &  15.26 \\
		MainlyBad (k=81)  & 5412.9 & 61.21 &     87.56 &  469.9 &                  15.43 & 1980.1 &                   15.43 &  19.18 \\
		MainlyBad (k=81)  & 5411.3 & 65.52 &     87.63 &  459.6 &                  19.84 & 1990.4 &                   19.84 &  18.76 \\
		TitForTat         & 5387.7 & 27.79 &     74.93 & 1422.7 &                  14.49 & 1027.3 &                   14.49 &  58.07 \\
		MainlyBad (k=70)  & 5387.1 & 62.40 &     80.90 &  728.7 &                  17.36 & 1721.3 &                   17.36 &  29.74 \\
		MainlyBad (k=97)  & 5386.9 & 65.11 &     97.66 &   74.5 &                   6.52 & 2375.5 &                    6.52 &   3.04 \\
		TitForTat         & 5386.4 & 26.80 &     74.50 & 1425.1 &                  11.69 & 1024.9 &                   11.69 &  58.17 \\
		TitForTat         & 5385.2 & 34.27 &     74.64 & 1422.7 &                  14.85 & 1027.3 &                   14.85 &  58.07 \\
		TitForTat         & 5383.9 & 32.66 &     74.98 & 1420.8 &                  13.31 & 1029.2 &                   13.31 &  57.99 \\
		TitForTat         & 5380.6 & 32.89 &     74.62 & 1421.3 &                  13.60 & 1028.7 &                   13.60 &  58.01 \\
		MainlyBad (k=99)  & 5379.4 & 61.92 &     99.21 &   26.3 &                   5.77 & 2423.7 &                    5.77 &   1.07 \\
		TitForTat         & 5367.2 & 22.34 &     74.74 & 1414.7 &                   9.78 & 1035.3 &                    9.78 &  57.74 \\
		TitForTat         & 5366.2 & 27.54 &     74.79 & 1415.7 &                  10.52 & 1034.3 &                   10.52 &  57.78 \\
		Bad               & 5362.4 & 78.10 &    100.00 &    0.0 &                   0.00 & 2450.0 &                    0.00 &   0.00 \\
		Bad               & 5359.2 & 41.10 &    100.00 &    0.0 &                   0.00 & 2450.0 &                    0.00 &   0.00 \\
		MainlyBad (k=98)  & 5352.7 & 35.40 &     98.74 &   48.7 &                   5.46 & 2401.3 &                    5.46 &   1.99 \\
		Bad               & 5343.2 & 41.91 &    100.00 &    0.0 &                   0.00 & 2450.0 &                    0.00 &   0.00 \\
		Bad               & 5330.8 & 33.93 &    100.00 &    0.0 &                   0.00 & 2450.0 &                    0.00 &   0.00 \\
		Bad               & 5322.0 & 21.00 &    100.00 &    0.0 &                   0.00 & 2450.0 &                    0.00 &   0.00 \\
		Indifferent       & 5258.2 & 90.87 &     69.41 & 1218.6 &                  28.88 & 1231.4 &                   28.88 &  49.74 \\
		Indifferent       & 5247.8 & 88.62 &     69.52 & 1219.9 &                  21.74 & 1230.1 &                   21.74 &  49.79 \\
		Indifferent       & 5246.0 & 66.33 &     69.21 & 1221.6 &                  15.01 & 1228.4 &                   15.01 &  49.86 \\
		GrimTrigger       & 5231.7 & 89.53 &     95.55 &  196.1 &                   0.32 & 2253.9 &                    0.32 &   8.00 \\
		Indifferent       & 5226.7 & 48.77 &     68.55 & 1222.9 &                  21.45 & 1227.1 &                   21.45 &  49.91 \\
		GrimTrigger       & 5223.5 & 79.12 &     95.58 &  196.1 &                   0.32 & 2253.9 &                    0.32 &   8.00 \\
		Indifferent       & 5223.1 & 62.10 &     69.24 & 1217.9 &                  29.42 & 1232.1 &                   29.42 &  49.71 \\
		Indifferent       & 5222.4 & 66.27 &     68.29 & 1220.1 &                  32.81 & 1229.9 &                   32.81 &  49.80 \\
		Indifferent       & 5216.1 & 82.14 &     70.41 & 1223.4 &                  26.90 & 1226.6 &                   26.90 &  49.93 \\
		GrimTrigger       & 5213.7 & 42.01 &     95.47 &  196.2 &                   0.63 & 2253.8 &                    0.63 &   8.01 \\
		GrimTrigger       & 5192.0 & 45.06 &     95.54 &  196.4 &                   0.97 & 2253.6 &                    0.97 &   8.02 \\
		GrimTrigger       & 5185.0 & 62.03 &     95.59 &  196.7 &                   1.16 & 2253.3 &                    1.16 &   8.03 \\
		TitFor2Tat        & 5148.6 & 16.61 &     70.94 & 1594.7 &                  10.07 &  855.3 &                   10.07 &  65.09 \\
		TitFor2Tat        & 5144.3 & 18.15 &     70.58 & 1599.4 &                  11.92 &  850.6 &                   11.92 &  65.28 \\
		TitFor2Tat        & 5139.4 & 18.24 &     70.47 & 1598.6 &                  18.54 &  851.4 &                   18.54 &  65.25 \\
		TitFor2Tat        & 5137.9 & 22.20 &     70.43 & 1586.5 &                  14.67 &  863.5 &                   14.67 &  64.76 \\
		TitFor2Tat        & 5136.4 & 34.72 &     70.42 & 1589.9 &                  19.76 &  860.1 &                   19.76 &  64.89 \\
		TitFor2Tat        & 5136.4 & 18.00 &     70.65 & 1590.7 &                   8.56 &  859.3 &                    8.56 &  64.93 \\
		TitFor2Tat        & 5121.5 & 29.34 &     70.77 & 1582.2 &                  17.42 &  867.8 &                   17.42 &  64.58 \\
		MainlyNice (k=42) & 5110.5 & 70.63 &     65.41 & 1421.1 &                  25.18 & 1028.9 &                   25.18 &  58.00 \\
		Nice              & 4948.5 & 38.81 &     46.50 & 2450.0 &                   0.00 &    0.0 &                    0.00 & 100.00 \\
		Nice              & 4926.0 & 43.24 &     46.05 & 2450.0 &                   0.00 &    0.0 &                    0.00 & 100.00 \\
		MainlyNice (k=17) & 4651.7 & 68.18 &     50.54 & 2033.3 &                  20.59 &  416.7 &                   20.59 &  82.99 \\
		MainlyNice (k=8)  & 4396.3 & 43.95 &     44.46 & 2260.2 &                  10.91 &  189.8 &                   10.91 &  92.25 \\
		MainlyNice (k=3)  & 4278.4 & 28.71 &     42.19 & 2379.4 &                   9.16 &   70.6 &                    9.16 &  97.12 \\
		Nice              & 4208.7 & 28.55 &     40.36 & 2450.0 &                   0.00 &    0.0 &                    0.00 & 100.00 \\
		Nice              & 4207.5 & 42.83 &     40.81 & 2450.0 &                   0.00 &    0.0 &                    0.00 & 100.00 \\
		Nice              & 4206.6 & 46.10 &     40.81 & 2450.0 &                   0.00 &    0.0 &                    0.00 & 100.00 \\
		Nice              & 4205.1 & 32.53 &     40.61 & 2450.0 &                   0.00 &    0.0 &                    0.00 & 100.00 \\ \bottomrule
	\end{tabular}
\end{table}

\begin{table}[ht]
   	\caption{8-players IPD, sorted by points, statistics}
   	\label{tab:ipdmp8}
   	\centering
    \begin{tabular}{l|rrr|rrrrr} \toprule
    	                  & \multicolumn{3}{c}{Points} & \multicolumn{2}{c}{Coop. count} & \multicolumn{2}{c}{Defect count} &        \\
    	Strategy          &   avg &   std &      yield &   avg &                     std &   avg &                      std &   C \% \\ \midrule
    	Bad               & 874.0 & 29.51 &     100.00 &   0.0 &                    0.00 & 350.0 &                     0.00 &   0.00 \\
    	GrimTrigger       & 850.3 & 31.69 &      98.28 &   7.0 &                    0.00 & 343.0 &                     0.00 &   2.00 \\
    	MainlyBad (k=72)  & 818.1 & 31.31 &      80.80 &  95.0 &                    6.46 & 255.0 &                     6.46 &  27.14 \\
    	Indifferent       & 740.6 & 15.43 &      68.23 & 175.9 &                    5.43 & 174.1 &                     5.43 &  50.26 \\
    	TitForTat         & 729.8 & 11.55 &      77.10 & 178.9 &                    5.59 & 171.1 &                     5.59 &  51.11 \\
    	TitFor2Tat        & 680.5 & 16.99 &      71.16 & 213.5 &                    9.11 & 136.5 &                     9.11 &  61.00 \\
    	MainlyNice (k=27) & 671.3 & 16.29 &      54.68 & 256.8 &                    8.98 &  93.2 &                     8.98 &  73.37 \\
    	Nice              & 528.3 & 19.67 &      39.70 & 350.0 &                    0.00 &   0.0 &                     0.00 & 100.00 \\ \bottomrule
    \end{tabular}
\end{table}

\begin{table}[ht]
	\caption{rMPIPD, constant pop of 50, strategy evolution through repetitions}
	\label{tab:ripdmp-const}
	\centering
    \begin{tabular}{l|ccccc} \toprule
    	$\downarrow$ Strategy -- Iter $\rightarrow$  & 0 & 1 & 2  & 3  & 4  \\ \midrule
    	GrimTrigger       & 5 & 5 & 4  & 3  & 1  \\
    	TitForTwoTat      & 7 & 7 & 6  & 6  & 4  \\
    	TitForTat         & 8 & 8 & 13 & 24 & 32 \\
    	Nice              & 6 & 6 & 3  & 0  & 0  \\
    	MainlyNice (k=3)  & 1 & 1 & 1  & 0  & 0  \\
    	MainlyNice (k=8)  & 1 & 1 & 0  & 0  & 0  \\
    	MainlyNice (k=17) & 1 & 1 & 0  & 0  & 0  \\
    	MainlyNice (k=42) & 1 & 1 & 1  & 1  & 1  \\
    	Indifferent       & 7 & 7 & 3  & 0  & 0  \\
    	MainlyBad (k=70)  & 1 & 1 & 1  & 1  & 1  \\
    	MainlyBad (k=78)  & 1 & 1 & 2  & 1  & 1  \\
    	MainlyBad (k=81)  & 2 & 2 & 3  & 3  & 2  \\
    	MainlyBad (k=85)  & 1 & 1 & 1  & 1  & 1  \\
    	MainlyBad (k=97)  & 1 & 1 & 2  & 2  & 1  \\
    	MainlyBad (k=98)  & 1 & 1 & 1  & 1  & 1  \\
    	MainlyBad (k=99)  & 1 & 1 & 1  & 1  & 1  \\
    	Bad               & 5 & 5 & 8  & 6  & 4  \\ \bottomrule
    \end{tabular}
\end{table}

\begin{table}[ht]
	\caption{rMPIPD, increasing pop, strategy evolution through repetitions}
	\label{tab:ripdmp-incr}
	\centering
	\begin{tabular}{l|cccccc} \toprule
		$\downarrow$ Strategy -- Iter $\rightarrow$ & 0 &  1 &  2 &  3 &  4 &   5 \\ \midrule
		GrimTrigger       & 5 &  6 &  7 &  7 &  8 &   9 \\
		TitForTwoTat      & 7 &  8 & 15 & 24 & 37 &  58 \\
		TitForTat         & 8 & 14 & 28 & 52 & 94 & 164 \\
		Nice              & 6 &  6 &  7 &  7 &  8 &  11 \\
		MainlyNice (k=3)  & 1 &  1 &  1 &  1 &  1 &   1 \\
		MainlyNice (k=8)  & 1 &  1 &  1 &  1 &  1 &   2 \\
		MainlyNice (k=17) & 1 &  1 &  1 &  1 &  2 &   4 \\
		MainlyNice (k=42) & 1 &  1 &  1 &  2 &  2 &   3 \\
		Indifferent       & 7 & 11 & 16 & 23 & 41 &  53 \\
		MainlyBad (k=70)  & 1 &  2 &  4 &  6 &  9 &  12 \\
		MainlyBad (k=78)  & 1 &  2 &  2 &  4 &  6 &   6 \\
		MainlyBad (k=81)  & 2 &  3 &  5 &  8 & 11 &  12 \\
		MainlyBad (k=85)  & 1 &  2 &  4 &  8 &  9 &  10 \\
		MainlyBad (k=97)  & 1 &  2 &  4 &  6 &  6 &   6 \\
		MainlyBad (k=98)  & 1 &  2 &  3 &  3 &  3 &   3 \\
		MainlyBad (k=99)  & 1 &  2 &  3 &  3 &  3 &   3 \\
		Bad               & 5 &  8 & 10 & 13 & 13 &  13 \\ \bottomrule
	\end{tabular}
\end{table}

\textbf{cipdmp to be added}

\end{document}
